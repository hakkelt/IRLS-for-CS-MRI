{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra, NBInclude, IterativeSolvers, FunctionOperators, Printf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rand (generic function with 166 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@nbinclude(\"helper_functions.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(123);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla MatrixIRLS for matrix completion (PCA) with p = 0\n",
    "\n",
    "https://mediatum.ub.tum.de/doc/1521436/1521436.pdf\n",
    "\n",
    "_**Note:** Vanilla = Conjugate gradient step (2.90) is solved directly without reducing the problem to a lower-dimensional projection space._\n",
    "\n",
    "<span style=\"color:red\"><strong>Problem:</strong> I don't know how should I calculate the solution of the constrained problem of arg $min_x \\langle X,W^{k-1}(X)\\rangle \\text{ s.t. } \\Phi(X) = y$ with conjugate gradient.</span>\n",
    "\n",
    "**Possible solutions:**\n",
    "- **[Linearly Constrained Least Squares](https://lls.readthedocs.io/en/latest/math.html):** In our case, $$min_x \\Vert W^{1/2} x - b \\Vert_2 \\text{ s.t. } \\Phi x = y$$ can be solved as $$\\begin{bmatrix} 2 W^{1/2 *} W^{1/2} & \\Phi^*\\\\ \\Phi & 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ z \\end{bmatrix} = \\begin{bmatrix} 2 W^{1/2} b \\\\ y \\end{bmatrix},$$ where $z$ is a new variable. This system has a unique solution when the matrix $\\Phi$ has independent rows (this is always true for sampling matrices), and the matrix $\\begin{bmatrix} A \\\\ C \\end{bmatrix}$ has indepedent columns (I'm not sure if it is always true).\n",
    "  - *The problem with this approach:*\n",
    "      - *I don't know how to calculate $W^{1/2}$ because it is not a matrix, but rather a function (see 2.57 on page 73).*\n",
    "      - *This method requires x to be a vector and $W$ should also be modified accordingly. Again, as $W$ is not a matrix, I have no idea how can I transform $W$ to suit the vectorized problem.*\n",
    "      - *$b$ is unknown.*\n",
    "      - *[The only Julia package](https://github.com/davidlizeng/LinearLeastSquares.jl) I found that solves the problem in this way is really outdated, and accepts only matrices.*\n",
    "- **Augmented Lagrangian and ADMM**\n",
    "  - *The problem with this approach:*\n",
    "      - *All of the solvers require $x$ to be a vector and $W$ should also be modified accordingly, but I don't know how it should be done.*\n",
    "      - *The solvers I found either support multi-variable (i.e. matrix) optimization or deal with general matrix-like/function-like objects like $W$, but I couldn't find any which does both.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vanilla_MatrixIRLS_for_PCA (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function vanilla_MatrixIRLS_for_PCA(\n",
    "        Xᴳᵀ::AbstractArray,                     # ground truth for MSE evaluation\n",
    "        y::AbstractArray,                       # under-sampled data\n",
    "        Φ::FunctionOperator;                    # sampling operator\n",
    "        img_size::NTuple = size(Xᴳᵀ),           # size of output matrix\n",
    "        r̃::Int = 0,                             # rank estimate of solution\n",
    "        maxIter::Union{Int, Nothing} = nothing, # number of CG iteration steps\n",
    "        N::Int = 10,                            # number of iterations\n",
    "        verbose::Bool = false)                  # print rank and loss value in each iteration\n",
    "    \n",
    "    # Initialize variables\n",
    "    dType = eltype(y)\n",
    "    d₁, d₂ = img_size\n",
    "    r̃ == 0 && (r̃ = rank(Xᴳᵀ))\n",
    "    maxIter = maxIter isa Nothing ? r̃*(r̃+d₁+d₂) : maxIter\n",
    "    ϵᵏ = Inf\n",
    "    Xᵏ = Φ' * y\n",
    "    σ = nothing # I just want to make it available outside of the loop\n",
    "    \n",
    "    for k in 1:N\n",
    "\"\"\"\n",
    "    2. Find best rank-(r̃ + 1) approximation of Xᵏ to obtain\n",
    "        𝒯ᵣ(Xᵏ) = Uᵏ * diag(σᵢᵏ)ᵢ₌₁ʳ * Vᵏ' and σᵣ₊₁ᵏ \n",
    "\"\"\"\n",
    "        F = svd(Xᵏ)\n",
    "        Uᵏ, σ, Vᵏ = F.U[:, 1:r̃], F.S, F.V[:, 1:r̃]\n",
    "        \n",
    "\"\"\"     update smoothing:                                 (2.91) \"\"\"\n",
    "        ϵᵏ = min(ϵᵏ, σ[r̃+1])\n",
    "        \n",
    "        r, n, s, e = rank(Xᵏ, atol = 1e-3), opnorm(Xᴳᵀ - Xᵏ, 2), σ[1], ϵᵏ\n",
    "        n, s, e = @sprintf(\"%7.3f\", n), @sprintf(\"%7.3f\", s), @sprintf(\"%7.3f\", e)\n",
    "        verbose && println(\"k = $(k-1),\\trank(Xᵏ) = $r,\\t‖Xᴳᵀ - Xᵏ‖₂ = $n, σ₁ = $s, ϵᵏ = $e\")\n",
    "        \n",
    "\"\"\"\n",
    "    3. Update Wᵏ as in (2.57), using parameters ϵ = ϵᵏ and p in (2.58) and (2.59), and the\n",
    "        information Uᵏ , Vᵏ and σ₁ᵏ, ..., σᵣ₊₁ᵏ from item 2.\n",
    "\n",
    "        (Lines below are based on Remark 2.3.2, the special case for p = 0)\n",
    "\"\"\"\n",
    "        Hᵏ = [1 / (max(σ[i], ϵᵏ) * max(σ[j], ϵᵏ))  for i in 1:r̃+1, j in 1:r̃+1]\n",
    "        Wᵏ = FunctionOperator{dType}(name = \"Wᵏ\", inDims = (d₁, d₂), outDims = (d₁, d₂),\n",
    "            forw = Z -> Uᵏ * (Hᵏ .* (Uᵏ' * Z * Vᵏ)) * Vᵏ')\n",
    "        \n",
    "\"\"\"\n",
    "    1. Use a conjugate gradient method to solve linearly constrained quadratic program\n",
    "         Xᵏ = arg minₓ ⟨X,Wᵏ⁻¹(X)⟩ s.t. Φ(X) = y         (2.90)\n",
    "\"\"\"\n",
    "        CG_op = throw(ErrorException(\"How should I calculate the solution of the constrained problem of \" *\n",
    "                                    \"arg minₓ ⟨X,Wᵏ⁻¹(X)⟩ s.t. Φ(X) = y with conjugate gradient?\"))\n",
    "        Xᵏ = cg(CG_op, b, maxiter = maxIter)\n",
    "    end\n",
    "    \n",
    "    r, n, s, e = rank(Xᵏ, atol = 1e-3), opnorm(Xᴳᵀ - Xᵏ, 2), σ[1], ϵᵏ\n",
    "    n, s, e = @sprintf(\"%7.3f\", n), @sprintf(\"%7.3f\", s), @sprintf(\"%7.3f\", e)\n",
    "    verbose && println(\"k = $N,\\trank(Xᵏ) = $r,\\t‖Xᴳᵀ - Xᵏ‖₂ = $n, σ₁ = $s, ϵᵏ = $e\")\n",
    "    \n",
    "    Xᵏ\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Experiments\n",
    "\n",
    "### General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix dimensions\n",
    "d₁, d₂ = 50, 50\n",
    "n = min(d₁, d₂)\n",
    "# Rank and number of non-zero elements in sparse component\n",
    "r, k = 7, 0\n",
    "# Type of matrix elements\n",
    "dType = ComplexF64;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding and Enhancing Data Recovery Algorithms\n",
    "\n",
    "*From Noise-Blind Sparse Recovery to Reweighted Methods for Low-Rank Matrix Optimization*\n",
    "\n",
    "*by Christian Kümmerle*\n",
    "\n",
    "https://mediatum.ub.tum.de/doc/1521436/1521436.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Low Rank Matrix\n",
    "Corresponding Matlab function: https://github.com/ckuemmerle/hm_irls/blob/master/sample_X0_lowrank.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size(L₀) = (50, 50)\n",
      "rank(L₀) = 7\n"
     ]
    }
   ],
   "source": [
    "L₀ = generateLowRankComponent_Christian(d₁, d₂, r, dType)\n",
    "@show size(L₀)\n",
    "@show rank(L₀);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling Mask ($\\Phi$)\n",
    "Corresponding Matlab function: https://github.com/ckuemmerle/hm_irls/blob/master/sample_phi_MatrixCompletion.m\n",
    "\n",
    "_**Note:** There is a difference in the way how the Christian's Matlab function and my Julia function satisfies the requirement of having at least $r$ non-zero entries in each row and each column._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r = 7\n",
      "minimum number of non-zero entries in each column: 9\n",
      "minimum number of non-zero entries in each column: 8\n"
     ]
    }
   ],
   "source": [
    "df = r * (d₁ + d₂ - r) # Number of degrees of freedom of the setting\n",
    "m = floor(Int, min(1.05 * df, d₁ * d₂))\n",
    "Φᴹ = generateΦ(d₁, d₂, r, m)\n",
    "Φ = FunctionOperator{dType}(name = \"Φ\", inDims = (d₁, d₂), outDims = (d₁, d₂),\n",
    "    forw = (b,x) -> b .= Φᴹ .* x, backw = (b,x) -> b .= x)\n",
    "@show r\n",
    "println(\"minimum number of non-zero entries in each column: \", Int(minimum(sum(Φᴹ, dims=1))))\n",
    "println(\"minimum number of non-zero entries in each column: \", Int(minimum(sum(Φᴹ, dims=2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subsampling The Ground Truth Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank(y) = 50\n"
     ]
    }
   ],
   "source": [
    "y = Φ * L₀\n",
    "@show rank(y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running The Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 0,\trank(Xᵏ) = 50,\t‖Xᴳᵀ - Xᵏ‖₂ = 174.811, σ₁ =  74.748, ϵᵏ =  33.581\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "How should I calculate the solution of the constrained problem of arg minₓ ⟨X,Wᵏ⁻¹(X)⟩ s.t. Φ(X) = y with conjugate gradient?",
     "output_type": "error",
     "traceback": [
      "How should I calculate the solution of the constrained problem of arg minₓ ⟨X,Wᵏ⁻¹(X)⟩ s.t. Φ(X) = y with conjugate gradient?",
      "",
      "Stacktrace:",
      " [1] vanilla_MatrixIRLS_for_PCA(::Array{Complex{Float64},2}, ::Array{Complex{Float64},2}, ::FunctionOperator{Complex{Float64}}; img_size::Tuple{Int64,Int64}, r̃::Int64, maxIter::Nothing, N::Int64, verbose::Bool) at ./In[4]:49",
      " [2] top-level scope at ./timing.jl:174 [inlined]",
      " [3] top-level scope at ./In[9]:0",
      " [4] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "@time vanilla_MatrixIRLS_for_PCA(L₀, y, Φ, N = 48, verbose = true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Principal Component Analysis?\n",
    "*by Emmanuel J. Candès, Xiaodong Li, Yi Ma, and John Wright*  \n",
    "https://arxiv.org/pdf/0912.3599.pdf\n",
    "\n",
    "#### 4.1 Exact recovery from varying fractions of error\n",
    "\n",
    "We first verify the correct recovery phenomenon of Theorem 1.1 on randomly generated problems. We consider square matrices of varying dimension $n = 500, \\ldots , 3000$. We generate a rank-$r$ matrix $L_0$ as a product $L_0 = XY^∗$ where $X$ and $Y$ are $n \\times r$ matrices with entries independently sampled\n",
    "from a $\\mathcal{N}(0,1/n)$ distribution. $S_0$ is generated by choosing a support set $\\Omega$ of size $k$ uniformly at random, and setting $S_0 = \\mathcal{P}_\\Omega E$, where $E$ is a matrix with independent Bernoulli $\\pm 1$ entries. Table 1 (top) reports the results with $r = rank(L_0) = 0.05 \\times n$ and $k = \\Vert S_0 \\Vert_0 = 0.05 \\times n^2$. Table 1 (bottom) reports the results for a more challenging scenario, $rank(L_0) = 0.05 \\times n$ and $k = 0.10 \\times n^2$. In all cases, we set $\\lambda = 1 \\cdot \\sqrt{n}$. Notice that in all cases, solving the convex PCP gives a result $(L, S)$ with the correct rank and sparsity. Moreover, the relative error $\\frac{\\Vert L - L_0 \\Vert_F}{\\Vert L_0 \\Vert_F}$ is small, less than $10^{-5}$ in all examples considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"table_1.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data\n",
    "\n",
    "_**Note:** In this notebook we deal only with PCA (simple matrix completion); therefore, there is no sparse component in the ground truth matrix._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size(L₀) = (50, 50)\n",
      "rank(L₀) = 7\n"
     ]
    }
   ],
   "source": [
    "L₀ = generateLowRankComponent_Candes(n, r, dType)\n",
    "@show size(L₀)\n",
    "@show rank(L₀);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling Mask ($\\Phi$)\n",
    "\n",
    "Using the earlier generated sampling mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subsampling The Ground Truth Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank(y) = 50\n"
     ]
    }
   ],
   "source": [
    "y = Φ * L₀\n",
    "@show rank(y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running The Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 0,\trank(Xᵏ) = 44,\t‖Xᴳᵀ - Xᵏ‖₂ =   0.044, σ₁ =   0.020, ϵᵏ =   0.011\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "How should I calculate the solution of the constrained problem of arg minₓ ⟨X,Wᵏ⁻¹(X)⟩ s.t. Φ(X) = y with conjugate gradient?",
     "output_type": "error",
     "traceback": [
      "How should I calculate the solution of the constrained problem of arg minₓ ⟨X,Wᵏ⁻¹(X)⟩ s.t. Φ(X) = y with conjugate gradient?",
      "",
      "Stacktrace:",
      " [1] vanilla_MatrixIRLS_for_PCA(::Array{Complex{Float64},2}, ::Array{Complex{Float64},2}, ::FunctionOperator{Complex{Float64}}; img_size::Tuple{Int64,Int64}, r̃::Int64, maxIter::Nothing, N::Int64, verbose::Bool) at ./In[4]:49",
      " [2] top-level scope at ./timing.jl:174 [inlined]",
      " [3] top-level scope at ./In[12]:0",
      " [4] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "@time vanilla_MatrixIRLS_for_PCA(L₀, y, Φ, N = 80, verbose = true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
