{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra, NBInclude, IterativeSolvers, FunctionOperators, Printf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rand (generic function with 166 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@nbinclude(\"helper_functions.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(123);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla MatrixIRLS for matrix completion (PCA) with p = 0\n",
    "\n",
    "https://mediatum.ub.tum.de/doc/1521436/1521436.pdf\n",
    "\n",
    "_**Note:** Vanilla = Conjugate gradient step (2.90) is solved directly without reducing the problem to a lower-dimensional projection space._\n",
    "\n",
    "<span style=\"color:red\"><strong>Problem:</strong> I don't know how should I calculate the solution of the constrained problem of arg $min_x \\langle X,W^{k-1}(X)\\rangle \\text{ s.t. } \\Phi(X) = y$ with conjugate gradient.</span>\n",
    "\n",
    "**Possible solutions:**\n",
    "- **[Linearly Constrained Least Squares](https://lls.readthedocs.io/en/latest/math.html):** In our case, $$min_x \\Vert W^{1/2} x - b \\Vert_2 \\text{ s.t. } \\Phi x = y$$ can be solved as $$\\begin{bmatrix} 2 W^{1/2 *} W^{1/2} & \\Phi^*\\\\ \\Phi & 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ z \\end{bmatrix} = \\begin{bmatrix} 2 W^{1/2} b \\\\ y \\end{bmatrix},$$ where $z$ is a new variable. This system has a unique solution when the matrix $\\Phi$ has independent rows (this is always true for sampling matrices), and the matrix $\\begin{bmatrix} A \\\\ C \\end{bmatrix}$ has indepedent columns (I'm not sure if it is always true).\n",
    "  - *The problem with this approach:*\n",
    "      - *I don't know how to calculate $W^{1/2}$ because it is not a matrix, but rather a function (see 2.57 on page 73).*\n",
    "      - *This method requires x to be a vector and $W$ should also be modified accordingly. Again, as $W$ is not a matrix, I have no idea how can I transform $W$ to suit the vectorized problem.*\n",
    "      - *$b$ is unknown.*\n",
    "      - *[The only Julia package](https://github.com/davidlizeng/LinearLeastSquares.jl) I found that solves the problem in this way is really outdated, and accepts only matrices.*\n",
    "- **Augmented Lagrangian and ADMM**\n",
    "  - *The problem with this approach:*\n",
    "      - *All of the solvers require $x$ to be a vector and $W$ should also be modified accordingly, but I don't know how it should be done.*\n",
    "      - *The solvers I found either support multi-variable (i.e. matrix) optimization or deal with general matrix-like/function-like objects like $W$, but I couldn't find any which does both.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vanilla_MatrixIRLS_for_PCA (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function vanilla_MatrixIRLS_for_PCA(\n",
    "        X·¥≥·µÄ::AbstractArray,                     # ground truth for MSE evaluation\n",
    "        y::AbstractArray,                       # under-sampled data\n",
    "        Œ¶::FunctionOperator;                    # sampling operator\n",
    "        img_size::NTuple = size(X·¥≥·µÄ),           # size of output matrix\n",
    "        rÃÉ::Int = 0,                             # rank estimate of solution\n",
    "        maxIter::Union{Int, Nothing} = nothing, # number of CG iteration steps\n",
    "        N::Int = 10,                            # number of iterations\n",
    "        verbose::Bool = false)                  # print rank and loss value in each iteration\n",
    "    \n",
    "    # Initialize variables\n",
    "    dType = eltype(y)\n",
    "    d‚ÇÅ, d‚ÇÇ = img_size\n",
    "    rÃÉ == 0 && (rÃÉ = rank(X·¥≥·µÄ))\n",
    "    maxIter = maxIter isa Nothing ? rÃÉ*(rÃÉ+d‚ÇÅ+d‚ÇÇ) : maxIter\n",
    "    œµ·µè = Inf\n",
    "    X·µè = Œ¶' * y\n",
    "    œÉ = nothing # I just want to make it available outside of the loop\n",
    "    \n",
    "    for k in 1:N\n",
    "\"\"\"\n",
    "    2. Find best rank-(rÃÉ + 1) approximation of X·µè to obtain\n",
    "        ùíØ·µ£(X·µè) = U·µè * diag(œÉ·µ¢·µè)·µ¢‚Çå‚ÇÅ ≥ * V·µè' and œÉ·µ£‚Çä‚ÇÅ·µè \n",
    "\"\"\"\n",
    "        F = svd(X·µè)\n",
    "        U·µè, œÉ, V·µè = F.U[:, 1:rÃÉ], F.S, F.V[:, 1:rÃÉ]\n",
    "        \n",
    "\"\"\"     update smoothing:                                 (2.91) \"\"\"\n",
    "        œµ·µè = min(œµ·µè, œÉ[rÃÉ+1])\n",
    "        \n",
    "        r, n, s, e = rank(X·µè, atol = 1e-3), opnorm(X·¥≥·µÄ - X·µè, 2), œÉ[1], œµ·µè\n",
    "        n, s, e = @sprintf(\"%7.3f\", n), @sprintf(\"%7.3f\", s), @sprintf(\"%7.3f\", e)\n",
    "        verbose && println(\"k = $(k-1),\\trank(X·µè) = $r,\\t‚ÄñX·¥≥·µÄ - X·µè‚Äñ‚ÇÇ = $n, œÉ‚ÇÅ = $s, œµ·µè = $e\")\n",
    "        \n",
    "\"\"\"\n",
    "    3. Update W·µè as in (2.57), using parameters œµ = œµ·µè and p in (2.58) and (2.59), and the\n",
    "        information U·µè , V·µè and œÉ‚ÇÅ·µè, ..., œÉ·µ£‚Çä‚ÇÅ·µè from item 2.\n",
    "\n",
    "        (Lines below are based on Remark 2.3.2, the special case for p = 0)\n",
    "\"\"\"\n",
    "        H·µè = [1 / (max(œÉ[i], œµ·µè) * max(œÉ[j], œµ·µè))  for i in 1:rÃÉ+1, j in 1:rÃÉ+1]\n",
    "        W·µè = FunctionOperator{dType}(name = \"W·µè\", inDims = (d‚ÇÅ, d‚ÇÇ), outDims = (d‚ÇÅ, d‚ÇÇ),\n",
    "            forw = Z -> U·µè * (H·µè .* (U·µè' * Z * V·µè)) * V·µè')\n",
    "        \n",
    "\"\"\"\n",
    "    1. Use a conjugate gradient method to solve linearly constrained quadratic program\n",
    "         X·µè = arg min‚Çì ‚ü®X,W·µè‚Åª¬π(X)‚ü© s.t. Œ¶(X) = y         (2.90)\n",
    "\"\"\"\n",
    "        CG_op = throw(ErrorException(\"How should I calculate the solution of the constrained problem of \" *\n",
    "                                    \"arg min‚Çì ‚ü®X,W·µè‚Åª¬π(X)‚ü© s.t. Œ¶(X) = y with conjugate gradient?\"))\n",
    "        X·µè = cg(CG_op, b, maxiter = maxIter)\n",
    "    end\n",
    "    \n",
    "    r, n, s, e = rank(X·µè, atol = 1e-3), opnorm(X·¥≥·µÄ - X·µè, 2), œÉ[1], œµ·µè\n",
    "    n, s, e = @sprintf(\"%7.3f\", n), @sprintf(\"%7.3f\", s), @sprintf(\"%7.3f\", e)\n",
    "    verbose && println(\"k = $N,\\trank(X·µè) = $r,\\t‚ÄñX·¥≥·µÄ - X·µè‚Äñ‚ÇÇ = $n, œÉ‚ÇÅ = $s, œµ·µè = $e\")\n",
    "    \n",
    "    X·µè\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Experiments\n",
    "\n",
    "### General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix dimensions\n",
    "d‚ÇÅ, d‚ÇÇ = 50, 50\n",
    "n = min(d‚ÇÅ, d‚ÇÇ)\n",
    "# Rank and number of non-zero elements in sparse component\n",
    "r, k = 7, 0\n",
    "# Type of matrix elements\n",
    "dType = ComplexF64;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding and Enhancing Data Recovery Algorithms\n",
    "\n",
    "*From Noise-Blind Sparse Recovery to Reweighted Methods for Low-Rank Matrix Optimization*\n",
    "\n",
    "*by Christian K√ºmmerle*\n",
    "\n",
    "https://mediatum.ub.tum.de/doc/1521436/1521436.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Low Rank Matrix\n",
    "Corresponding Matlab function: https://github.com/ckuemmerle/hm_irls/blob/master/sample_X0_lowrank.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size(L‚ÇÄ) = (50, 50)\n",
      "rank(L‚ÇÄ) = 7\n"
     ]
    }
   ],
   "source": [
    "L‚ÇÄ = generateLowRankComponent_Christian(d‚ÇÅ, d‚ÇÇ, r, dType)\n",
    "@show size(L‚ÇÄ)\n",
    "@show rank(L‚ÇÄ);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling Mask ($\\Phi$)\n",
    "Corresponding Matlab function: https://github.com/ckuemmerle/hm_irls/blob/master/sample_phi_MatrixCompletion.m\n",
    "\n",
    "_**Note:** There is a difference in the way how the Christian's Matlab function and my Julia function satisfies the requirement of having at least $r$ non-zero entries in each row and each column._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r = 7\n",
      "minimum number of non-zero entries in each column: 9\n",
      "minimum number of non-zero entries in each column: 8\n"
     ]
    }
   ],
   "source": [
    "df = r * (d‚ÇÅ + d‚ÇÇ - r) # Number of degrees of freedom of the setting\n",
    "m = floor(Int, min(1.05 * df, d‚ÇÅ * d‚ÇÇ))\n",
    "Œ¶·¥π = generateŒ¶(d‚ÇÅ, d‚ÇÇ, r, m)\n",
    "Œ¶ = FunctionOperator{dType}(name = \"Œ¶\", inDims = (d‚ÇÅ, d‚ÇÇ), outDims = (d‚ÇÅ, d‚ÇÇ),\n",
    "    forw = (b,x) -> b .= Œ¶·¥π .* x, backw = (b,x) -> b .= x)\n",
    "@show r\n",
    "println(\"minimum number of non-zero entries in each column: \", Int(minimum(sum(Œ¶·¥π, dims=1))))\n",
    "println(\"minimum number of non-zero entries in each column: \", Int(minimum(sum(Œ¶·¥π, dims=2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subsampling The Ground Truth Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank(y) = 50\n"
     ]
    }
   ],
   "source": [
    "y = Œ¶ * L‚ÇÄ\n",
    "@show rank(y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running The Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 0,\trank(X·µè) = 50,\t‚ÄñX·¥≥·µÄ - X·µè‚Äñ‚ÇÇ = 174.811, œÉ‚ÇÅ =  74.748, œµ·µè =  33.581\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "How should I calculate the solution of the constrained problem of arg min‚Çì ‚ü®X,W·µè‚Åª¬π(X)‚ü© s.t. Œ¶(X) = y with conjugate gradient?",
     "output_type": "error",
     "traceback": [
      "How should I calculate the solution of the constrained problem of arg min‚Çì ‚ü®X,W·µè‚Åª¬π(X)‚ü© s.t. Œ¶(X) = y with conjugate gradient?",
      "",
      "Stacktrace:",
      " [1] vanilla_MatrixIRLS_for_PCA(::Array{Complex{Float64},2}, ::Array{Complex{Float64},2}, ::FunctionOperator{Complex{Float64}}; img_size::Tuple{Int64,Int64}, rÃÉ::Int64, maxIter::Nothing, N::Int64, verbose::Bool) at ./In[4]:49",
      " [2] top-level scope at ./timing.jl:174 [inlined]",
      " [3] top-level scope at ./In[9]:0",
      " [4] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "@time vanilla_MatrixIRLS_for_PCA(L‚ÇÄ, y, Œ¶, N = 48, verbose = true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Principal Component Analysis?\n",
    "*by Emmanuel J. Cand√®s, Xiaodong Li, Yi Ma, and John Wright*  \n",
    "https://arxiv.org/pdf/0912.3599.pdf\n",
    "\n",
    "#### 4.1 Exact recovery from varying fractions of error\n",
    "\n",
    "We first verify the correct recovery phenomenon of Theorem 1.1 on randomly generated problems. We consider square matrices of varying dimension $n = 500, \\ldots , 3000$. We generate a rank-$r$ matrix $L_0$ as a product $L_0 = XY^‚àó$ where $X$ and $Y$ are $n \\times r$ matrices with entries independently sampled\n",
    "from a $\\mathcal{N}(0,1/n)$ distribution. $S_0$ is generated by choosing a support set $\\Omega$ of size $k$ uniformly at random, and setting $S_0 = \\mathcal{P}_\\Omega E$, where $E$ is a matrix with independent Bernoulli $\\pm 1$ entries. Table 1 (top) reports the results with $r = rank(L_0) = 0.05 \\times n$ and $k = \\Vert S_0 \\Vert_0 = 0.05 \\times n^2$. Table 1 (bottom) reports the results for a more challenging scenario, $rank(L_0) = 0.05 \\times n$ and $k = 0.10 \\times n^2$. In all cases, we set $\\lambda = 1 \\cdot \\sqrt{n}$. Notice that in all cases, solving the convex PCP gives a result $(L, S)$ with the correct rank and sparsity. Moreover, the relative error $\\frac{\\Vert L - L_0 \\Vert_F}{\\Vert L_0 \\Vert_F}$ is small, less than $10^{-5}$ in all examples considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"table_1.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data\n",
    "\n",
    "_**Note:** In this notebook we deal only with PCA (simple matrix completion); therefore, there is no sparse component in the ground truth matrix._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size(L‚ÇÄ) = (50, 50)\n",
      "rank(L‚ÇÄ) = 7\n"
     ]
    }
   ],
   "source": [
    "L‚ÇÄ = generateLowRankComponent_Candes(n, r, dType)\n",
    "@show size(L‚ÇÄ)\n",
    "@show rank(L‚ÇÄ);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling Mask ($\\Phi$)\n",
    "\n",
    "Using the earlier generated sampling mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subsampling The Ground Truth Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank(y) = 50\n"
     ]
    }
   ],
   "source": [
    "y = Œ¶ * L‚ÇÄ\n",
    "@show rank(y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running The Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 0,\trank(X·µè) = 44,\t‚ÄñX·¥≥·µÄ - X·µè‚Äñ‚ÇÇ =   0.044, œÉ‚ÇÅ =   0.020, œµ·µè =   0.011\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "How should I calculate the solution of the constrained problem of arg min‚Çì ‚ü®X,W·µè‚Åª¬π(X)‚ü© s.t. Œ¶(X) = y with conjugate gradient?",
     "output_type": "error",
     "traceback": [
      "How should I calculate the solution of the constrained problem of arg min‚Çì ‚ü®X,W·µè‚Åª¬π(X)‚ü© s.t. Œ¶(X) = y with conjugate gradient?",
      "",
      "Stacktrace:",
      " [1] vanilla_MatrixIRLS_for_PCA(::Array{Complex{Float64},2}, ::Array{Complex{Float64},2}, ::FunctionOperator{Complex{Float64}}; img_size::Tuple{Int64,Int64}, rÃÉ::Int64, maxIter::Nothing, N::Int64, verbose::Bool) at ./In[4]:49",
      " [2] top-level scope at ./timing.jl:174 [inlined]",
      " [3] top-level scope at ./In[12]:0",
      " [4] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "@time vanilla_MatrixIRLS_for_PCA(L‚ÇÄ, y, Œ¶, N = 80, verbose = true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
